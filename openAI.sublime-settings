{
    // Your openAI token
    "token": "",

    // Apply Sublime Text markdown syntax highligh to OpenAI completion output panel text.
    // Affects only `chat_completion` command.
    // `MultimarkdownEditing` package highly recommended to apply syntax highlight for a code snippets.
    "markdown": true,

    // Minimum amount of characters selected to perform completion.
    "minimum_selection_length": 20,

    // Status bar hint setup that presents major info about currently active assistant setup (from the array of assistnat objects above)
    // Possible options:
    //  - name: User defined assistant setup name
    //  - prompt_mode: Model output prompt mode (panel|append|insert|replace)
    //  - chat_model: Which OpenAI model are used within this setup (e.g. gpt-4, gpt-3.5-turbo-16k).
    //
    // You're capable to mix these whatewer you want and the text in status bar will follow.
    "status_hint": [
        // "name",
        // "prompt_mode",
        // "chat_model"
     ],

    // Proxy setting
    "proxy": {
        // Proxy address
        "address": "",

        // Proxy port
        "port": 8080
    },

    "assistants": [
        {
            // A string that will presented in command palete.
            // Those assistants that are purposed to be used as a function - complete, edit, summarize etc. -
            "name": "Default", // **REQUIRED**

            // Mode of how plugin should promts its output, available options:
            //  - panel: prompt would be output in output panel, selected text wouldn't be affected in any way.
            //  - append: prompt would be added next to the selected text.
            //  - insert: prompt would be inserted instead of a placeholder within a selected text.
            //  - replace: prompt would overwite selected text.
            //
            // All cases but `panel` required to some text be selected beforehand.
            // The same in all cases but `panel` user type within input panel will be treated by a model
            // as `system` command, e.g. instruction to action.
            "prompt_mode": "panel", // **REQUIRED**

            // The model which will generate the chat completion.
            // Generaly here should be either "gpt-3.5.-turbo|gpt-4|gpt-3.5-turbo-instruct" or their specified versions.
            // Learn more at https://beta.openai.com/docs/models
            "chat_model": "gpt-3.5-turbo", // **REQUIRED**

            // ChatGPT model knows how to role, lol
            // It can act as a different kind of person. Recently in this plugin it was acting
            // like as a code assistant. With this setting you're able to set it up more precisely.
            // E.g. "You are (rust|python|js|whatewer) developer assistant", "You are an english tutor" and so on.
            "assistant_role": "You are a senior code assitant", // **REQUIRED**

            // What sampling temperature to use, between 0 and 2.
            // Higher values like 0.8 will make the output more random,
            // while lower values like 0.2 will make it more focused and deterministic.
            //
            // OpenAI generally recommend altering this or `top_p` but not both.
            "temperature": 1,

            // The maximum number of tokens to generate in the completion.
            // The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
            // (One token is roughly 4 characters for normal English text)
            // Does not affect editing mode.
            "max_tokens": 2048,

            // An alternative to sampling with temperature, called nucleus sampling,
            // where the model considers the results of the tokens with `top_p` probability mass.
            // So 0.1 means only the tokens comprising the top 10% probability mass are considered.
            //
            // OpenAI generally recommend altering this or `temperature` but not both.
            "top_p": 1,

            // Number between -2.0 and 2.0.
            // Positive values penalize new tokens based on their existing frequency in the text so far,
            // decreasing the model's likelihood to repeat the same line verbatim.
            // docs: https://platform.openai.com/docs/api-reference/parameter-details
            "frequency_penalty": 0,

            // Number between -2.0 and 2.0.
            /// Positive values penalize new tokens based on whether they appear in the text so far,
            // increasing the model's likelihood to talk about new topics.
            // docs: https://platform.openai.com/docs/api-reference/parameter-details
            "presence_penalty": 0,
        }
    ],

    ////////// ----------------------------------- LEGACY ----------------------------------- //////////
    // The model which will generate the code edition.
    // Some models are suitable for natural language tasks, others specialize in code.
    // Learn more at https://beta.openai.com/docs/models
    // ____Affects only edition mode.____
    //
    // DEPRECATED: Will become obsolete at version 3.0.0 on 01.01.2024.
    "edit_model": "code-davinci-edit-001",

    // The model which will generate the completion.
    // Some models are suitable for natural language tasks, others specialize in code.
    // Learn more at https://beta.openai.com/docs/models
    // ____Affects only completion and inserting modes.____
    //
    // DEPRECATED: Will become obsolete at version 3.0.0 on 01.01.2024.
    "model": "gpt-3.5-turbo-instruct",
    // Same as `temperature` in assistants array, but for backward capability with legacy OpenAI API
    //
    // Used for legacy `v1/completion` capability
    // DEPRECATED: Will become obsolete at version 3.0.0 on 01.01.2024.
    "temperature": 1,

    // Same as `max_tokens` in assistants array, but for backward capability with legacy OpenAI API
    //
    // Used for legacy `v1/completion` capability
    // DEPRECATED: Will become obsolete at version 3.0.0 on 01.01.2024.
    "max_tokens": 2048,

    // Same as `top_p` in assistants array, but for backward capability with legacy OpenAI API
    //
    // Used for legacy `v1/completion` capability
    // DEPRECATED: Will become obsolete at version 3.0.0 on 01.01.2024.
    "top_p": 1,

    // Same as `frequency_penalty` in assistants array, but for backward capability with legacy OpenAI API
    //
    // Used for legacy `v1/completion` capability
    // DEPRECATED: Will become obsolete at version 3.0.0 on 01.01.2024.
    "frequency_penalty": 0,

    // Same as `presence_penalty` in assistants array, but for backward capability with legacy OpenAI API
    //
    // Used for legacy `v1/completion` capability
    // DEPRECATED: Will become obsolete at version 3.0.0 on 01.01.2024.
    "presence_penalty": 0,
    ////////// ----------------------------------- LEGACY ----------------------------------- //////////
}