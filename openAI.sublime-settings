{
    // The model which will generate the code edition.
    // Some models are suitable for natural language tasks, others specialize in code.
    // Learn more at https://beta.openai.com/docs/models
    // ____Affects only edition mode.____
    //
    // DEPRECATED: would be deleted at 3.0.0
    "edit_model": "code-davinci-edit-001",

    // The model which will generate the completion.
    // Some models are suitable for natural language tasks, others specialize in code.
    // Learn more at https://beta.openai.com/docs/models
    // ____Affects only completion and inserting modes.____
    //
    // DEPRECATED: would be deleted at 3.0.0
    "model": "text-davinci-003",

    // The model which will generate the chat completion.
    // Some models are suitable for natural language tasks, others specialize in code.
    // Learn more at https://beta.openai.com/docs/models
    // ____Affects only chat completion mode___
    "chat_model": "gpt-3.5-turbo",

    // ChatGPT model knows how to role, lol
    // It can act as a different kind of person. Recently in this plugin it was acting
    // like as a code assistant. With this setting you're able to set it up more precisely.
    // E.g. "You are (rust|python|js|whatewer) developer assistant", "You are an english tutor" and so on.
    //
    // DEPRECATED: would be deleted at 3.0.0
    "assistant_role": "You are a senior code assitant",

    // What sampling temperature to use, between 0 and 2.
    // Higher values like 0.8 will make the output more random,
    // while lower values like 0.2 will make it more focused and deterministic.
    //
    // OpenAI generally recommend altering this or top_p but not both.
    "temperature": 1,

    // The maximum number of tokens to generate in the completion.
    // The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
    // (One token is roughly 4 characters for normal English text)
    // Does not affect editing mode.
    "max_tokens": 256,

    // An alternative to sampling with temperature, called nucleus sampling,
    // where the model considers the results of the tokens with `top_p` probability mass.
    // So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    // OpenAI generally recommend altering this or temperature but not both.
    "top_p": 1,

    // Number between -2.0 and 2.0.
    // Positive values penalize new tokens based on their existing frequency in the text so far,
    // decreasing the model's likelihood to repeat the same line verbatim.
    // docs: https://platform.openai.com/docs/api-reference/parameter-details
    "frequency_penalty": 0,

    // Number between -2.0 and 2.0.
    /// Positive values penalize new tokens based on whether they appear in the text so far,
    // increasing the model's likelihood to talk about new topics.
    // docs: https://platform.openai.com/docs/api-reference/parameter-details
    "presence_penalty": 0,

    // Placeholder for insert mode. You should to put it where you want the suggestion to be inserted.
    // e.g. (python)
    // def get_bitcoin_price():
    //    [insert]
    //    print(bitcoin_price)
    "placeholder": "[insert]",

    // Your openAI token
    "token": "",

    // Apply Sublime Text markdown syntax highligh to OpenAI completion output panel text.
    // Affects only `chat_completion` command.
    // `MultimarkdownEditing` package highly recommended to apply syntax highlight for a code snippets.
    "markdown": true,

    // Minimum amount of characters selected to perform completion.
    "minimum_selection_length": 20,

    // Proxy setting
    "proxy": {
        // Proxy address
        "address": "",

        // Proxy port
        "port": 8080
    }
}